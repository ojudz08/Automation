{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e8952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCK PRICE 1: Get PSE stock price using requests library\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import dateutil.parser\n",
    "\n",
    "directory = r'C:\\Users\\ojell\\Desktop\\Oj\\_Thesis\\Data\\stock_price'\n",
    "\n",
    "base_url = r'https://pselookup.vrymel.com/api'\n",
    "sym = 'AC'\n",
    "per = ['2021-01-04', '2021-01-05']\n",
    "\n",
    "response = requests.get(base_url + f'/stocks/{sym}/history/{per[0]}/{per[1]}')\n",
    "data_json = response.json()\n",
    "df = pd.DataFrame(data_json['history'])\n",
    "df['timestamp'] = df['timestamp'].apply(lambda x: dateutil.parser.isoparse(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "#df.to_csv(directory + r'\\ayala_test.csv', index=False)   # Comment out to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = r'https://www.bworldonline.com/category/top-stories/'\n",
    "\n",
    "headers = headers={'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'}\n",
    "cont = requests.get(url, headers).content\n",
    "\n",
    "soup = BeautifulSoup(cont, \"html.parser\")\n",
    "\n",
    "test = soup.find(\"ul\", {\"id\": \"menu-header-menu-1\"}).find_all(\"a\")\n",
    "\n",
    "category = [\"Top Stories\"]\n",
    "\n",
    "ctgry_url = []\n",
    "for item in range(0, len(test)):\n",
    "    for i in range(0, len(category)):\n",
    "        if test[item].text.lower() == category[i].lower():\n",
    "            ctgry_url.append(test[item].get(\"href\"))\n",
    "            \n",
    "#print(ctgry_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90737761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCK PRICE 2: Get PSE stock price using requests library from API\n",
    "#                https://pselookup.vrymel.com/api\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class PseStockPrice():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dir = r'C:\\Users\\ojell\\Desktop\\Oj\\_Thesis\\Data\\stock_price'\n",
    "        self.pse = 'pse_company_all.xls'\n",
    "        self.api_url = r'https://pselookup.vrymel.com/api'\n",
    "        self.period = ['2000-01-01', '2021-07-30']\n",
    "\n",
    "\n",
    "    def pseFile(self):\n",
    "        file = os.path.join(self.dir, self.pse)\n",
    "        if os.path.exists(file):\n",
    "            output = pd.read_excel(file)\n",
    "        else:\n",
    "            output = f\"Check if {self.pse} exist\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    def pseAPI(self):\n",
    "        df = self.pseFile()\n",
    "        data = df.copy()    #Remove .head() once you've created the error function\n",
    "\n",
    "        symbol = data['Stock Symbol']\n",
    "        df_all = pd.DataFrame()\n",
    "        for i in range(0, len(data)):\n",
    "            response = requests.get(self.api_url + f'/stocks/{symbol[i]}/history/{self.period[0]}/{self.period[1]}')\n",
    "            api = pd.DataFrame(response.json()['history'])\n",
    "            comp = pd.concat([pd.DataFrame(data.iloc[i, :]).T] * len(api), ignore_index=True)\n",
    "            comb_data = pd.concat([comp, api], axis=1)\n",
    "\n",
    "            #comb_data.to_csv(self.dir + f'\\{symbol[i]}_historical_data.csv', index=False)\n",
    "\n",
    "            # NOTE: Commented out since there's an error when concatenating all data\n",
    "            df_all = df_all.append(comb_data)\n",
    "\n",
    "        # NOTE: Commented out since there's an error when concatenating all data\n",
    "        df_all.to_csv(self.dir + r'\\pse_stock_price_all.csv', index=False)\n",
    "\n",
    "        msg = \"Done pulling out data!\"\n",
    "\n",
    "        return msg\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test = PseStockPrice()\n",
    "    #print(test.pseAPI())   # --> COMMENT OUT IF YOU WANT TO PULL OUT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "72140166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NEWS 1: Get news articles and headline using requests, lxml and Beautifulsoup module from\n",
    "#         Business World: https://www.bworldonline.com/\n",
    "#         Inquirer.Net: https://www.inquirer.net/\n",
    "#         BusinessMirror: https://businessmirror.com.ph/\n",
    "#         Philippine News Agency: https://www.pna.gov.ph/\n",
    "#         The Manila Times: https://www.manilatimes.net/\n",
    "#         CNN PH: https://cnnphilippines.com/\n",
    "#         PhilStar: https://www.philstar.com/\n",
    "#         Manila Bulletin: https://mb.com.ph/\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class bwOnline():\n",
    "    \n",
    "    def __init__(self, news_url, category, page):\n",
    "        self.n_url = news_url\n",
    "        self.cat = category\n",
    "        self.pg = page\n",
    "    \n",
    "    def parse(self, url):\n",
    "        headers = {'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'}\n",
    "        cont = requests.get(url, headers).content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "        return soup\n",
    "        \n",
    "    def categoryUrl(self):\n",
    "        sp = self.parse(self.n_url).find(\"ul\", {\"id\": \"menu-header-menu-1\"}).find_all(\"a\")\n",
    "        \n",
    "        ctgry_url = []            \n",
    "        for item in range(0, len(sp)):\n",
    "            if len(self.cat) == 1 and self.cat[0].lower() == 'all':\n",
    "                for i in range(0, len(self.cat)):\n",
    "                    ctgry_url.append(sp[item].get(\"href\"))\n",
    "            else:    \n",
    "                for i in range(0, len(self.cat)):\n",
    "                    if sp[item].text.lower() == self.cat[i].lower():\n",
    "                        ctgry_url.append(sp[item].get(\"href\"))\n",
    "        \n",
    "        return ctgry_url\n",
    "        \n",
    "    def articleUrl(self):         \n",
    "        ctgry = self.categoryUrl()       \n",
    "        page = self.pg      \n",
    "        \n",
    "        pgurl = []\n",
    "        art_url = []\n",
    "        for i in range(0, len(ctgry)):\n",
    "            if page == 'all': # get back on this\n",
    "                for a in range(1, 458):\n",
    "                    pgurl.append(f'{ctgry[i]}page/{a}/')\n",
    "            elif len(page) == 2:\n",
    "                for b in range(page[0], page[1] + 1):\n",
    "                    pgurl.append(f'{ctgry[i]}page/{b}/')\n",
    "            else:\n",
    "                for c in range(0, len(page)):\n",
    "                    pgurl.append(f'{ctgry[i]}page/{page[c]}/')\n",
    "            \n",
    "        for url_i in pgurl:\n",
    "            sp = self.parse(url_i).find(\"div\", {\"class\": \"td-ss-main-content\"}).find_all(\"div\", {\"class\": \"item-details\"})\n",
    "            for item in sp:\n",
    "                art_url.append(item.find(\"a\").get(\"href\"))\n",
    "        \n",
    "        return art_url\n",
    "    \n",
    "    def headlines(self):\n",
    "        url = self.articleUrl()\n",
    "        head = []\n",
    "        for i in range(0, len(url)):\n",
    "            head.append(self.parse(url[i]).find(\"h1\", {\"class\": \"entry-title\"}).text)\n",
    "        \n",
    "        return head\n",
    "    \n",
    "    def published(self):\n",
    "        url = self.articleUrl()\n",
    "        pub_date = []\n",
    "        for i in range(0, len(url)):\n",
    "            # For now focus on date, not time\n",
    "            pub_date.append(self.parse(url[i]).find(\"span\", {\"class\": \"td-post-date\"}).text.split(\" | \")[0])\n",
    "        \n",
    "        return pub_date\n",
    "    \n",
    "    def content(self):\n",
    "        url = self.articleUrl()\n",
    "        test = url[0]\n",
    "        \n",
    "        for i in range(0, len(url)):\n",
    "            content = self.parse(test).find(\"div\", {\"class\": \"td-post-content td-pb-padding-side\"}).text\n",
    "        \n",
    "        return content\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    news_url = r'https://www.bworldonline.com/'\n",
    "    category = [\"Corporate\"]\n",
    "    page = [1]\n",
    "    #date = []\n",
    "    \n",
    "    news = bwOnline(news_url, category, page)\n",
    "    \n",
    "    #test = news.content()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b171c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEWS 2: Modification of above script\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class bwOnline():\n",
    "    \n",
    "    def __init__(self, news_url, category, page):\n",
    "        self.n_url = news_url\n",
    "        self.cat = category\n",
    "        self.pg = page\n",
    "    \n",
    "    def parse(self, url):\n",
    "        headers = {'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'}\n",
    "        cont = requests.get(url, headers).content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')        \n",
    "        return soup\n",
    "        \n",
    "    def categoryUrl(self):\n",
    "        sp = self.parse(self.n_url).find(\"ul\", {\"id\": \"menu-header-menu-1\"}).find_all(\"a\")\n",
    "        ctgry_url = []\n",
    "        for item in range(0, len(sp)):\n",
    "            if len(self.cat) == 1 and self.cat[0].lower() == 'all':\n",
    "                for i in range(0, len(self.cat)):\n",
    "                    ctgry_url.append(sp[item].get(\"href\"))\n",
    "            else:    \n",
    "                for i in range(0, len(self.cat)):\n",
    "                    if sp[item].text.lower() == self.cat[i].lower():\n",
    "                        ctgry_url.append(sp[item].get(\"href\"))\n",
    "        \n",
    "        return ctgry_url\n",
    "    \n",
    "        \n",
    "    def articleUrl(self):         \n",
    "        ctgry = self.categoryUrl()       \n",
    "        page = self.pg      \n",
    "        pgurl = []\n",
    "        art_url = []\n",
    "        for i in range(0, len(ctgry)):\n",
    "            #lastpg = self.parse(ctgry[i]).find(\"a\", {\"class\": \"last\"}).text\n",
    "            \n",
    "            if page == 'all':\n",
    "                for a in range(1, 458):\n",
    "                    pgurl.append(f'{ctgry[i]}page/{a}/')\n",
    "            elif len(page) == 2:\n",
    "                for b in range(page[0], page[1] + 1):\n",
    "                    pgurl.append(f'{ctgry[i]}page/{b}/')\n",
    "            else:\n",
    "                for c in range(0, len(page)):\n",
    "                    pgurl.append(f'{ctgry[i]}page/{page[c]}/')\n",
    "            \n",
    "        for url_i in pgurl:\n",
    "            sp = self.parse(url_i).find(\"div\", {\"class\": \"td-ss-main-content\"}).find_all(\"div\", {\"class\": \"item-details\"})\n",
    "            for item in sp:\n",
    "                art_url.append(item.find(\"a\").get(\"href\"))\n",
    "        \n",
    "        return art_url\n",
    "    \n",
    "    def category(self):\n",
    "        sp = self.parse(self.n_url).find(\"ul\", {\"id\": \"menu-header-menu-1\"}).find_all(\"a\")\n",
    "        \n",
    "        ctgry = []\n",
    "        for item in range(0, len(sp)):\n",
    "            if len(self.cat) == 1 and self.cat[0].lower() == 'all':\n",
    "                for i in range(0, len(self.cat)):\n",
    "                    ctgry.append(sp[item].text)\n",
    "            else:    \n",
    "                for i in range(0, len(self.cat)):\n",
    "                    if sp[item].text.lower() == self.cat[i].lower():\n",
    "                        ctgry.append(sp[item].text)        \n",
    "        return ctgry\n",
    "    \n",
    "    def headlines(self):\n",
    "        url = self.articleUrl()\n",
    "        head = []\n",
    "        for i in range(0, len(url)):\n",
    "            head.append(self.parse(url[i]).find(\"h1\", {\"class\": \"entry-title\"}).text)\n",
    "        \n",
    "        return head\n",
    "    \n",
    "    def published(self):\n",
    "        url = self.articleUrl()\n",
    "        pub = []\n",
    "        for i in range(0, len(url)):\n",
    "            # For now focus on date, not time\n",
    "            pub.append(self.parse(url[i]).find(\"span\", {\"class\": \"td-post-date\"}).text.split(\" | \"))\n",
    "        \n",
    "        return pub\n",
    "    \n",
    "    def content(self):\n",
    "        url = self.articleUrl()\n",
    "        test = url[0]\n",
    "        \n",
    "        for i in range(0, len(url)):\n",
    "            content = self.parse(test).find(\"div\", {\"class\": \"td-post-content td-pb-padding-side\"}).text\n",
    "        \n",
    "        return content\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    news_url = r'https://www.bworldonline.com/'\n",
    "    category = [\"Corporate\"]\n",
    "    page = [1]\n",
    "    #date = []\n",
    "    \n",
    "    news = bwOnline(news_url, category, page)\n",
    "    \n",
    "    test = news.headlines()\n",
    "    \n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "233b0bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Lengths must match to compare', (22,), (1,))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-835fe5c585ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mnews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbwOnline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_ctgry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m#output = test.to_excel(os.path.join(save_path, \"news_output.xlsx\"), index=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-835fe5c585ba>\u001b[0m in \u001b[0;36mheadlines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m#idx = data_df[data_df['Date']==p].index[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallctgry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mallctgry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Category\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctgry\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\progpy\\anaconda3\\envs\\py_3_8\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\progpy\\anaconda3\\envs\\py_3_8\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\progpy\\anaconda3\\envs\\py_3_8\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m#  The ambiguous case is object-dtype.  See GH#27803\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[1;34m\"Lengths must match to compare\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: ('Lengths must match to compare', (22,), (1,))"
     ]
    }
   ],
   "source": [
    "# NEWS 3: Modification of above script\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "class parseNews():\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "class bwOnline():\n",
    "    \n",
    "    def __init__(self, news_url, category):\n",
    "        self.n_url = news_url\n",
    "        self.ctgry = category\n",
    "    \n",
    "    def parse(self, url):\n",
    "        headers = {'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'}\n",
    "        cont = requests.get(url, headers).content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')        \n",
    "        return soup\n",
    "        \n",
    "    def allCategory(self):\n",
    "        ct_sp = self.parse(self.n_url).find(\"ul\", {\"id\": \"menu-header-menu-1\"}).find_all(\"a\")\n",
    "        \n",
    "        # NOTE: Work on how to pull-out Health category\n",
    "        exclude = [\"BUYING RATES\", \"FOREIGN INTEREST RATES\", \"Philippine Mutual Funds\", \"Leaders and Laggards\", \n",
    "                   \"Stock Quotes\", \"Stock Markets Summary\", \"Non-BSP Convertible Currencies\", \"BSP Convertible Currencies\",\n",
    "                   \"Health\"]\n",
    "        \n",
    "        all_ctgry = [i.text for i in ct_sp if i.text not in exclude]\n",
    "        all_ctgry_url = [i.get(\"href\") for i in ct_sp if i.text not in exclude]\n",
    "        lstpg = [self.parse(all_ctgry_url[i]).find(\"a\", {\"class\": \"last\"}).text for i in range(0, len(all_ctgry_url))]\n",
    "        \n",
    "        dct = {\"Category\": all_ctgry, \"Category Url\": all_ctgry_url, \"Last Page\": lstpg}\n",
    "        output = pd.DataFrame(dct)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "        \n",
    "    def headlines(self):\n",
    "        allctgry = self.allCategory()\n",
    "        \n",
    "        #idx = data_df[data_df['Date']==p].index[0]\n",
    "        output = allctgry[allctgry[\"Category\"]==self.ctgry].index[0]\n",
    "    \n",
    "        \n",
    "        \n",
    "        return output\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    news_url = r'https://www.bworldonline.com/'\n",
    "    save_path = r'C:\\Users\\ojell\\Desktop\\Oj\\_Thesis\\Data\\news'\n",
    "    get_ctgry = [\"Corporate\"]\n",
    "    #dt_range = []\n",
    "    \n",
    "    news = bwOnline(news_url, get_ctgry)\n",
    "    \n",
    "    test = news.headlines()    \n",
    "    print(test)\n",
    "    #output = test.to_excel(os.path.join(save_path, \"news_output.xlsx\"), index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b507b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
